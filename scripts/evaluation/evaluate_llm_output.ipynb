{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import ast\n",
    "import Levenshtein\n",
    "import astor\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        return [json.loads(line) for line in lines]\n",
    "    \n",
    "def read_json_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def get_indentation_level(line):\n",
    "    return len(line) - len(line.lstrip())\n",
    "\n",
    "\n",
    "\n",
    "def get_project_path(project_name):\n",
    "    init_path = os.getcwd() + \"/benchmarks/CoderEval/CoderEval/home/travis/builds/repos\"\n",
    "    project = None\n",
    "    for root, dirs, files in os.walk(init_path):\n",
    "        for d in dirs:\n",
    "            if d == project_name:\n",
    "                project = os.path.join(root, d)\n",
    "                break\n",
    "        if project:\n",
    "            break\n",
    "    return project\n",
    "\n",
    "    \n",
    "def get_task_info(task_id, tasks_file_path=\"benchmarks/CoderEval/CoderEval4Python.json\"):\n",
    "    tasks = read_json_file(tasks_file_path)[\"RECORDS\"]\n",
    "\n",
    "    for task in tasks:\n",
    "        project_name, original_file_path, task_code, test_name, task_name = None, None, None, None, None\n",
    "        if task[\"_id\"] == task_id:\n",
    "            project_name, file_path, task_code, test_name, task_name = task[\"project\"].replace('/', '---'), task[\"file_path\"].replace('.py', '_test.py'), task[\"code\"], task[\"test_name\"], task[\"name\"]\n",
    "            original_file_path = file_path.replace('.py', f'_{task_id}.py')\n",
    "            break\n",
    "\n",
    "    complete_path = os.path.join(get_project_path(project_name), original_file_path)\n",
    "    if os.path.exists(complete_path):\n",
    "        test_file = open(complete_path, 'r')\n",
    "        test_content = test_file.read()\n",
    "        if test_name == \"\":\n",
    "            # replace the except block with an except Exception and print it before isT = False\n",
    "            test_content = test_content.replace('    except:\\n        isT = False', '    except Exception as e:\\n        isT = False\\n        print(\"Error while running the task: \", e)')\n",
    "            test_content = test_content.replace('    except: \\n        isT = False', '    except Exception as e:\\n        isT = False\\n        print(\"Error while running the task: \", e)')\n",
    "            test_content = test_content.replace('    except:\\n        isT=False', '    except Exception as e:\\n        isT = False\\n        print(\"Error while running the task: \", e)')\n",
    "            test_content = test_content.replace('    except:\\n        isT= False', '    except Exception as e:\\n        isT = False\\n        print(\"Error while running the task: \", e)')\n",
    "        test_file.close()\n",
    "    else:\n",
    "        test_contents = read_json_file(\"benchmarks/CoderEval/tests/record_testcases_map_python.json\")\n",
    "        for test_task_id, test_content in test_contents.items():\n",
    "            if test_task_id == task_id:\n",
    "                # add an assert for isT if not a test file\n",
    "                if test_name == \"\":\n",
    "                    test_content = test_content.replace(f'if not isT:\\n        raise Exception(\"Result not True!!!\")', '')\n",
    "                    test_content = test_content + f\"\\n    try:\\n        assert isT == True, \\\"isT is not True\\\"\\n        print(\\\"Tests passed for the task: {task_name}\\\")\\n    except AssertionError as e:\\n        print(\\\"Test failed for the task:  {task_name}\\\", e)\"\n",
    "                break\n",
    "        \n",
    "    return project_name, original_file_path, task_code, test_content, test_name, task_name\n",
    "\n",
    "\n",
    "def get_prompt_techniques_applied(is_zero, is_fewshot, is_CoT, is_persona, is_package, is_signature):\n",
    "    prompt_techinques_applied = \"\"\n",
    "    if is_zero:\n",
    "        prompt_techinques_applied += \"Zero-shot, \"\n",
    "    if is_fewshot:\n",
    "        prompt_techinques_applied += \"Few-shot, \"\n",
    "    if is_CoT:\n",
    "        prompt_techinques_applied += \"CoT, \"\n",
    "    if is_persona:\n",
    "        prompt_techinques_applied += \"Persona, \"\n",
    "    if is_package:\n",
    "        prompt_techinques_applied += \"Package, \"\n",
    "    if is_signature:\n",
    "        prompt_techinques_applied += \"Signature, \"\n",
    "    prompt_techinques_applied = prompt_techinques_applied[:-2]\n",
    "    return prompt_techinques_applied\n",
    "\n",
    "\n",
    "def extract_function_name(groundtruth_code):\n",
    "    tree = ast.parse(groundtruth_code)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            return node.name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_python_version_installed(version):\n",
    "    result = subprocess.run([\"pyenv\", \"versions\", \"--bare\"], capture_output=True, text=True)\n",
    "    installed_versions = result.stdout.split()\n",
    "    abstract_versions = [\".\".join(v.split(\".\")[:2]) for v in installed_versions]\n",
    "    print(abstract_versions)\n",
    "    return version in abstract_versions\n",
    "\n",
    "\n",
    "def get_complete_python_version(python_version):\n",
    "    result = subprocess.run([\"pyenv\", \"versions\", \"--bare\"], capture_output=True, text=True)\n",
    "    installed_versions = result.stdout.split()\n",
    "    for v in installed_versions:\n",
    "        if python_version in v:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_standard_library(module_name):\n",
    "    if module_name in sys.builtin_module_names:\n",
    "        return True\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(module_name)\n",
    "        if spec is None or spec.origin is None:\n",
    "            return False\n",
    "        module_path = spec.origin\n",
    "\n",
    "        # Get the standard library path\n",
    "        std_lib_path = os.path.dirname(os.__file__)\n",
    "        \n",
    "        # Check if the module path is within the standard library directory\n",
    "        if module_path.startswith(std_lib_path):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def check_module_needs_install(module_name):\n",
    "    modules_to_ignore = [\"re\", \"os\", \"sys\", \"subprocess\", \"urllib\", \"src\", \"collections\", \"-r\"]\n",
    "    if module_name == \"pytz\" or \"six\":\n",
    "        return True\n",
    "    if module_name.replace(' ', '') == \"\" or module_name in modules_to_ignore:\n",
    "        return False\n",
    "    if is_standard_library(module_name):\n",
    "        return False  # It's a standard library module\n",
    "    try:\n",
    "        print(\"importing \", module_name)\n",
    "        importlib.import_module(module_name)\n",
    "        return False  \n",
    "    except ImportError:\n",
    "        return True  # Module needs to be installed\n",
    "   \n",
    "\n",
    "def get_python_version(project_name):\n",
    "    with open(\"project_versions.csv\", mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            if row[0] == project_name:\n",
    "                return row[1].strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def install_python(python_version):\n",
    "    # Install the required Python version if not installed\n",
    "    if not is_python_version_installed(python_version):\n",
    "        print(f\"Installing Python {python_version}\")\n",
    "        if \"3.6\" in python_version:\n",
    "            print(\"Applying patch for Python 3.6.15\")\n",
    "            subprocess.run([\"pyenv\", \"install\", \"3.6.15\"], shell=True)\n",
    "        else:\n",
    "            subprocess.run([\"pyenv\", \"install\", python_version], shell=True)\n",
    "    \n",
    "    complete_python_version = get_complete_python_version(python_version)\n",
    "    print(\"Python installed: \", complete_python_version)\n",
    "    python_path = os.path.expanduser(f\"~/.pyenv/versions/{complete_python_version}/bin/python{python_version}\")\n",
    "    pip_path = os.path.expanduser(f\"~/.pyenv/versions/{complete_python_version}/bin/pip{python_version}\")#Users/ranimkhojah/.pyenv/versions/3.7.17/\n",
    "\n",
    "    return python_path, pip_path\n",
    "\n",
    "def install_general_dependencies(project_path, pip_path):\n",
    "    requirements_files = [\"requirements.txt\", \"test-requirements.txt\", \"requirements-dev.txt\", \"requirements.dev.txt\", \"requirements_dev.txt\", \"requirements-test.txt\", \"requirements-swh.txt\", \"requirements-dev.txt\", \"test_requirements.txt\", \"requirements-development.txt\", \"test-requirements.txt\"]\n",
    "    for req_file in requirements_files:\n",
    "        req_file_path = os.path.join(project_path, req_file)\n",
    "        if os.path.exists(req_file_path):\n",
    "            print(f\"Found requirements file: {req_file}\")\n",
    "            try:\n",
    "                install_requirements(req_file_path, pip_path)\n",
    "                # subprocess.check_call([pip_path, 'install', '-r', req_file_path])\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Failed to install requirements: Command failed with error {e}\")\n",
    "    \n",
    "    # check local dependencies (setup.py)\n",
    "    setup_file = os.path.join(project_path, \"setup.py\")\n",
    "    if os.path.exists(setup_file):\n",
    "        try:\n",
    "            subprocess.check_call([pip_path, 'install', '-e', project_path])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install local dependencies: Command failed with error {e}\")\n",
    "    \n",
    "def install_requirements(req_file_path, pip_path):\n",
    "    with open(req_file_path, 'r') as f:\n",
    "        packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([pip_path, 'install', package])\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"Failed to install {package}, continuing...\")\n",
    "\n",
    "\n",
    "def install_imports(task_file_path, pip_path):\n",
    "    dependencies = set()\n",
    "\n",
    "    with open(task_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('import ') or line.startswith('from '):\n",
    "                parts = line.split()\n",
    "                if parts[0] == 'import':\n",
    "                    dependencies.add(parts[1].split('.')[0])\n",
    "                elif parts[0] == 'from':\n",
    "                    dependencies.add(parts[1].split('.')[0])\n",
    "\n",
    "    # Filter out standard library modules\n",
    "    third_party_dependencies = {dep for dep in dependencies if check_module_needs_install(dep)}\n",
    "    # Install dependencies\n",
    "    for dep in third_party_dependencies:\n",
    "        # refactor\n",
    "        mapping = {\"yaml\": \"pyyaml\", \"ruamel\": \"ruamel.yaml\", \"git\": \"GitPython\", \"OpenSSL\": \"pyOpenSSL\", \"requests\": \"requests==2.25.1\", \"Crypto\": \"pycryptodome\", \"PIL\": \"Pillow\", \"fs_s3fs\": \"fs-s3fs\", \"dateutil\": \"python-dateutil\"}\n",
    "        if dep in mapping:\n",
    "            dep = mapping[dep]\n",
    "        try:\n",
    "            if dep == \"six\":\n",
    "                subprocess.check_call([pip_path, 'install', '--upgrade', 'setuptools<36'])\n",
    "                os.environ[\"VIRTUALENV_NO_DOWNLOAD\"] = \"1\"\n",
    "\n",
    "            subprocess.check_call([pip_path, 'install', dep])\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {dep}: Command failed with error {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def initialize_venv():\n",
    "    subprocess.run(['pyenv', 'init', '--path'], shell=True)\n",
    "    subprocess.run(['pyenv', 'init', '-'], shell=True)\n",
    "    subprocess.run(['pyenv', 'rehash'], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pytest(python_path, pip_path, project, project_name):\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONPATH\"] = python_path\n",
    "    subprocess.run([pip_path, \"install\", \"pytest\"], shell=False, cwd=project)\n",
    "    # subprocess.run([pip_path, \"install\", \"-e\", \".\"], shell=False, cwd=project)\n",
    "    if project_name == \"awsteiner---o2sclpy\": # special case for h5py library weird installation\n",
    "        subprocess.run(['brew', 'install', 'hdf5'], shell=False, cwd=project)\n",
    "        subprocess.run([pip_path, \"install\", \"--no-build-isolation\", \"--no-cache-dir\", \"h5py\"], shell=False, cwd=project)\n",
    "        subprocess.run([pip_path, \"install\", \"h5py\"], shell=False, cwd=project)\n",
    "               \n",
    "\n",
    "def levenshtein_distance(str1, str2):\n",
    "    return Levenshtein.distance(str1, str2)\n",
    "\n",
    "def jaccard_distance(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    return 1 - len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def compute_distance(str1, str2, method=\"jaccard\"):\n",
    "    if method == \"jaccard\":\n",
    "        return jaccard_distance(str1, str2)\n",
    "    elif method == \"levenshtein\":\n",
    "        return levenshtein_distance(str1, str2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid similarity method\")\n",
    "\n",
    "\n",
    "def remove_docstring(function_str):\n",
    "    tree = ast.parse(function_str)        \n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            if (node.body and isinstance(node.body[0], ast.Expr) and\n",
    "            isinstance(node.body[0].value, (ast.Str, ast.Constant))):\n",
    "            # Remove the docstring\n",
    "                node.body = node.body[1:]\n",
    "    \n",
    "    new_function_str = astor.to_source(tree)\n",
    "    \n",
    "    return new_function_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_and_tasks = {}\n",
    "\n",
    "with open(\"benchmarks/CoderEval/CoderEval4Python.json\") as f:\n",
    "    data = json.load(f)\n",
    "    for record in data[\"RECORDS\"]:\n",
    "        project_name = record[\"project\"].replace('/', '---')\n",
    "        task_id = record[\"_id\"]\n",
    "        if project_name not in projects_and_tasks:\n",
    "            projects_and_tasks[project_name] = []\n",
    "        projects_and_tasks[project_name].append(task_id)\n",
    "\n",
    "print(\"Number of projects: \", len(projects_and_tasks))\n",
    "print(\"Number of Python tasks\", len(data[\"RECORDS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project_name, tasks in projects_and_tasks.items():\n",
    "    if project_name != \"\":\n",
    "        continue\n",
    "    print(\"Preparing the environment for project: \", project_name)\n",
    "\n",
    "    project = get_project_path(project_name)\n",
    "\n",
    "    python_version = get_python_version(project_name)\n",
    "    \n",
    "\n",
    "    if os.path.exists(os.path.join(project, f\"{project_name}_env\")):\n",
    "        print(f\"Virtual environment already exists for {project_name}\")\n",
    "        # get existing pip and python paths\n",
    "        python_path = os.path.join(project, f\"{project_name}_env/bin/python\")\n",
    "        pip_path = os.path.join(project, f\"{project_name}_env/bin/pip\")\n",
    "    else:\n",
    "        python_path, pip_path = install_python(python_version)\n",
    "        # pip_path = os.path.join(project, f\"{project_name}_env/bin/pip{python_version}\")\n",
    "        initialize_venv()\n",
    "        print(f\"Creating virtual environment in Python {python_version}...\")\n",
    "        subprocess.run([python_path, \"-m\", \"venv\", f\"{project_name}_env\"], cwd=project)\n",
    "    \n",
    "    print(\"Installing dependencies ...\")\n",
    "    install_general_dependencies(project, pip_path)\n",
    "\n",
    "    # # Uncomment to install dependencies for each task\n",
    "    # for task_id in tasks:\n",
    "    #     print(\"Installing dependencies for task: \", task_id)\n",
    "    #     project_name, file_path, task_code, test_file, test_name, task_name = get_task_info(task_id)\n",
    "    #     complete_file_path = os.path.join(project, file_path)\n",
    "    #     install_imports(complete_file_path, pip_path)\n",
    "    #     print(\"Dependencies installed for task: \", task_name, \" (\" , task_id, \")\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'codereval'\n",
    "language = 'python'\n",
    "version = '12'\n",
    "model = 'mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "\n",
    "stats_per_tech = {}\n",
    "stats_per_task = {}\n",
    "\n",
    "model_results_path = 'results/' + model + '/'\n",
    "model_output_file = model_results_path + 'model_output/' + model + '_' + dataset + '_' + language + '_v' + version + '.jsonl'\n",
    "model_predictions = read_jsonl_file(model_output_file)\n",
    "evaluation_results_file = model_results_path + 'evaluation_results/evaluation_' + dataset +'_v' + version + '.csv'\n",
    "\n",
    "with open(evaluation_results_file, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['comb_id', 'task_id', 'prompt_technique', 'prompt', 'test_result', 'error_message', 'groundtruth_code', 'generated_code', 'lexical_distance', 'test_code', 'is_zero', 'is_fewshot', 'is_CoT', 'is_persona', 'is_package', 'is_signature']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter='$')\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, row in enumerate(model_predictions):\n",
    "        try:\n",
    "            # about the prediction of tasks\n",
    "            task_id = str(row['task_id'])\n",
    "            is_zero, is_fewshot, is_CoT, is_persona, is_package, is_signature = row['combination']\n",
    "            combination = ''.join([str(int(is_zero)), str(int(is_fewshot)), str(int(is_CoT)), str(int(is_persona)), str(int(is_package)), str(int(is_signature))])\n",
    "            prompt_techinques_applied = get_prompt_techniques_applied(is_zero, is_fewshot, is_CoT, is_persona, is_package, is_signature)\n",
    "            prompt = re.sub(r'\\n', '', row['prompt'])\n",
    "\n",
    "            # about the project and groundtruth\n",
    "            groundtruth_code = row['groundtruth_code']\n",
    "            test_code = row['tests'] # just a part of the test, just for printing\n",
    "\n",
    "            project_name, original_file_path, task_code, test_content, test_name, task_name = get_task_info(task_id)\n",
    "\n",
    "            # if project_name != \"pexip---os-zope\":\n",
    "            #     continue\n",
    "\n",
    "            project = get_project_path(project_name)\n",
    "            python_version = get_python_version(project_name)\n",
    "            python_path = os.path.join(project, f\"{project_name}_env/bin/python\")\n",
    "            pip_path = os.path.join(project, f\"{project_name}_env/bin/pip\")\n",
    "            \n",
    "            # about the generated code\n",
    "            generated_code = row['generated_code']\n",
    "            if generated_code == \"\" or generated_code == \"null\" or generated_code == None:\n",
    "                print(f\"Skipping task {task_id} with combination {combination} as the generated code is empty\")\n",
    "                writer.writerow({'comb_id': i, 'task_id': task_id, 'prompt_technique': prompt_techinques_applied, 'prompt': prompt, 'test_result': 'Failed', 'error_message': 'Incomplete code generated', 'groundtruth_code': groundtruth_code, 'generated_code': generated_code, 'lexical_distance': '', 'test_code': test_code, 'is_zero': is_zero, 'is_fewshot': is_fewshot, 'is_CoT': is_CoT, 'is_persona': is_persona, 'is_package': is_package, 'is_signature': is_signature})\n",
    "                continue\n",
    "\n",
    "            file_path = original_file_path.replace('.py', f'_{combination}.py')\n",
    "            file_path = os.path.join(project, file_path)\n",
    "\n",
    "            # creating the test file\n",
    "            with open(file_path, \"w\") as f:\n",
    "                groundtruth_code_function_name = extract_function_name(groundtruth_code)\n",
    "                generated_code_function_name = extract_function_name(generated_code)\n",
    "\n",
    "                # ensure the correct indentation level\n",
    "                dedented_generated_code = textwrap.dedent(generated_code)\n",
    "                first_line = task_code.splitlines()[0]\n",
    "                indentation_level = get_indentation_level(first_line)\n",
    "                generated_task_code = dedented_generated_code.replace(generated_code_function_name, groundtruth_code_function_name)\n",
    "                generated_task_code = textwrap.indent(generated_task_code, ' ' * indentation_level) + \"\\n\\n\"\n",
    "\n",
    "                test_content = test_content.replace(task_code, generated_task_code) \n",
    "                f.write(test_content)\n",
    "\n",
    "\n",
    "            print(f\"Running the task: {task_id} ({task_name}) with combination: {combination}\")\n",
    "\n",
    "            if test_name == \"\": # This is a class not a test file\n",
    "                class_output = subprocess.run([python_path, file_path], capture_output=True, text=True, cwd=project)\n",
    "                if \"Error while running the task:\" in class_output.stdout:\n",
    "                    extracted_error = re.search(r'Error while running the task: (.*)', class_output.stdout)\n",
    "                    class_output.stderr = extracted_error.group(1)\n",
    "                elif \"isT is not True\" in class_output.stdout:\n",
    "                    class_output.stderr = \"isT is not True\"\n",
    "            else:\n",
    "                print(\"This is a pytest file.\")\n",
    "                setup_pytest(python_path, pip_path, project, project_name)\n",
    "                class_output = subprocess.run([python_path, \"-m\", \"pytest\", file_path], capture_output=True, text=True, cwd=project)\n",
    "\n",
    "                if class_output.returncode == 0:\n",
    "                    class_output.stdout = class_output.stdout + \"Tests passed for the task: \" + task_id\n",
    "                    \n",
    "\n",
    "            print(\"Class output:\", class_output.stdout)\n",
    "            print(\"Class error:\", class_output.stderr)\n",
    "\n",
    "\n",
    "            # groundtruth_code = remove_docstring(groundtruth_code)\n",
    "            # generated_code = remove_docstring(generated_code)\n",
    "            # similarity_between_codes = compute_distance(groundtruth_code, generated_code, method=\"levenshtein\")\n",
    "\n",
    "            test_result = \"Passed\" if \"Tests passed for the task\" in class_output.stdout else \"Failed\"\n",
    "\n",
    "            writer.writerow({'comb_id': i, 'task_id': task_id, 'prompt_technique': prompt_techinques_applied, 'prompt': prompt, 'test_result': test_result, 'error_message': class_output.stderr, 'groundtruth_code': groundtruth_code, 'generated_code': generated_code, 'lexical_distance': '', 'test_code': test_code, 'is_zero': is_zero, 'is_fewshot': is_fewshot, 'is_CoT': is_CoT, 'is_persona': is_persona, 'is_package': is_package, 'is_signature': is_signature})\n",
    "\n",
    "\n",
    "            if test_result == \"Failed\":\n",
    "                if task_id in stats_per_task:\n",
    "                    stats_per_task[task_id] += 1\n",
    "                else:\n",
    "                    stats_per_task[task_id] = 1\n",
    "                if prompt_techinques_applied in stats_per_tech:\n",
    "                    stats_per_tech[prompt_techinques_applied] += 1\n",
    "                else:\n",
    "                    stats_per_tech[prompt_techinques_applied] = 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error running the task: \", task_id, e)\n",
    "            continue\n",
    "\n",
    "\n",
    "print(\"Failed tests per prompt technique: \\n\")\n",
    "print(stats_per_tech)\n",
    "print(\"Failed tests per task_id: \\n\")\n",
    "print(stats_per_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_projects_output_{version}.txt', 'w') as file:\n",
    "    file.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
